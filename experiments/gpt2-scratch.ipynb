{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GXs_RippsGL",
        "outputId": "f4e1eb25-be59-4067-fd19-9aadb741711b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
              " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model(**encoded_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "211972"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datasets\n",
        "train_10 = datasets.load_dataset(\"roneneldan/TinyStories\", split=\"train[:10%]\")\n",
        "len(train_10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00d7f7f5cf0c4d2ea525222ac965d770",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=8):   0%|          | 0/211972 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenization(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        max_length=model.config.max_position_embeddings,\n",
        "        truncation=True,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "\n",
        "# Batched tokenization. Can feed into model for outputs/logits to be softmaxed.\n",
        "ds = train_10.map(\n",
        "    tokenization,\n",
        "    batched=True,\n",
        "    num_proc=8,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids'],\n",
              "    num_rows: 211972\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
              " [3198,\n",
              "  1110,\n",
              "  11,\n",
              "  257,\n",
              "  1310,\n",
              "  2576,\n",
              "  3706,\n",
              "  20037,\n",
              "  1043,\n",
              "  257,\n",
              "  17598,\n",
              "  287,\n",
              "  607,\n",
              "  2119,\n",
              "  13,\n",
              "  1375,\n",
              "  2993,\n",
              "  340,\n",
              "  373,\n",
              "  2408,\n",
              "  284,\n",
              "  711,\n",
              "  351,\n",
              "  340,\n",
              "  780,\n",
              "  340,\n",
              "  373,\n",
              "  7786,\n",
              "  13,\n",
              "  20037,\n",
              "  2227,\n",
              "  284,\n",
              "  2648,\n",
              "  262,\n",
              "  17598,\n",
              "  351,\n",
              "  607,\n",
              "  1995,\n",
              "  11,\n",
              "  523,\n",
              "  673,\n",
              "  714,\n",
              "  34249,\n",
              "  257,\n",
              "  4936,\n",
              "  319,\n",
              "  607,\n",
              "  10147,\n",
              "  13,\n",
              "  198,\n",
              "  198,\n",
              "  43,\n",
              "  813,\n",
              "  1816,\n",
              "  284,\n",
              "  607,\n",
              "  1995,\n",
              "  290,\n",
              "  531,\n",
              "  11,\n",
              "  366,\n",
              "  29252,\n",
              "  11,\n",
              "  314,\n",
              "  1043,\n",
              "  428,\n",
              "  17598,\n",
              "  13,\n",
              "  1680,\n",
              "  345,\n",
              "  2648,\n",
              "  340,\n",
              "  351,\n",
              "  502,\n",
              "  290,\n",
              "  34249,\n",
              "  616,\n",
              "  10147,\n",
              "  1701,\n",
              "  2332,\n",
              "  1995,\n",
              "  13541,\n",
              "  290,\n",
              "  531,\n",
              "  11,\n",
              "  366,\n",
              "  5297,\n",
              "  11,\n",
              "  20037,\n",
              "  11,\n",
              "  356,\n",
              "  460,\n",
              "  2648,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  4259,\n",
              "  534,\n",
              "  10147,\n",
              "  526,\n",
              "  198,\n",
              "  198,\n",
              "  41631,\n",
              "  11,\n",
              "  484,\n",
              "  4888,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  384,\n",
              "  19103,\n",
              "  262,\n",
              "  4936,\n",
              "  319,\n",
              "  20037,\n",
              "  338,\n",
              "  10147,\n",
              "  13,\n",
              "  632,\n",
              "  373,\n",
              "  407,\n",
              "  2408,\n",
              "  329,\n",
              "  606,\n",
              "  780,\n",
              "  484,\n",
              "  547,\n",
              "  7373,\n",
              "  290,\n",
              "  5742,\n",
              "  1123,\n",
              "  584,\n",
              "  13,\n",
              "  2293,\n",
              "  484,\n",
              "  5201,\n",
              "  11,\n",
              "  20037,\n",
              "  26280,\n",
              "  607,\n",
              "  1995,\n",
              "  329,\n",
              "  7373,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  18682,\n",
              "  607,\n",
              "  10147,\n",
              "  13,\n",
              "  1119,\n",
              "  1111,\n",
              "  2936,\n",
              "  3772,\n",
              "  780,\n",
              "  484,\n",
              "  550,\n",
              "  4888,\n",
              "  290,\n",
              "  3111,\n",
              "  1978,\n",
              "  13])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds[0][\"text\"], ds[0][\"input_ids\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(162, 132)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ds[0][\"input_ids\"]), len(ds[0][\"text\"].split(\" \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'input_ids': [3041, 5372], 'attention_mask': [1, 1]},\n",
              " {'input_ids': [1326], 'attention_mask': [1]},\n",
              " {'input_ids': [1525], 'attention_mask': [1]},\n",
              " {'input_ids': [3041, 5372, 502, 416], 'attention_mask': [1, 1, 1, 1]},\n",
              " GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              " })"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Replace\"), tokenizer(\"me\"), tokenizer(\"by\"), tokenizer(\"Replace me by\"), tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Re', 'place', 'Ġme', 'Ġby', 'Ġany', 'Ġtext'],\n",
              " ['Re', 'place', 'Ġme', 'Ġby', 'Ġany', 'Ġtext', 'Ġyou', \"'d\", 'Ġlike', '.'])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens([3041, 5372, 502, 416, 597, 2420]), tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'One day, a little girl named Lily found a needle in her room. She jumped out, grabbed a few items, pulled up the cover, got'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. It was an old tube in her back, with her eyes closed, her'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. She immediately had a good dream — a very different one.\\n\\nHer'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. She was taken to the hospital and treated with antibiotics.\\n\\nShe has'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room.\\n\\n\"What,\" she said, holding out her hand to stop the'}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator(\"One day, a little girl named Lily found a needle in her room.\", max_length=30, num_return_sequences=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 10, 768]), 46.90026, -12.414162, 755, 1.0, 496)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "tmp = output.last_hidden_state[0][0].detach().numpy()\n",
        "output.last_hidden_state.shape, tmp.max(), tmp.min(), ((tmp > -1) & (tmp < 1)).sum(), scipy.special.softmax(tmp).sum(), scipy.special.softmax(tmp).argmax()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BeamSearchDecoderOnlyOutput(sequences=tensor([[ 1212,   318,   257, 14343,  1621,    13,   198,   198,    40,  1101,\n",
              "           407,  1654,   644,   284,   787,   286,   340,    13,   198,   198],\n",
              "        [ 1212,   318,   257, 14343,  1621,    13,   198,   198,    40,  1101,\n",
              "           407,  1654,   644,   284,   787,   286,   340,    13,   314,  1101]]), sequences_scores=tensor([-17.3244, -19.0733]), scores=(tensor([[-12.6087, -11.0740, -12.2497,  ..., -23.2802, -27.1528,  -5.2005],\n",
              "        [-12.6087, -11.0740, -12.2497,  ..., -23.2802, -27.1529,  -5.2005]]), tensor([[-13.2075, -12.2395, -17.4515,  ..., -21.1790, -14.0837, -14.2907],\n",
              "        [-15.6107,  -8.7284, -13.3961,  ..., -32.1316, -39.7618, -10.3586]]), tensor([[-12.7155,  -2.6471,  -6.8343,  ..., -26.4551, -30.0658, -13.9922],\n",
              "        [-14.3286, -12.3563, -17.0929,  ..., -19.5023, -18.9123, -14.7603]]), tensor([[-13.0612,  -8.4863, -10.2133,  ..., -22.5126, -26.4204,  -9.6500],\n",
              "        [-12.1657, -11.5912, -15.1831,  ..., -20.0144, -13.1187, -12.7388]]), tensor([[-13.2222, -10.9737, -17.2112,  ..., -20.3124, -13.6227, -14.1369],\n",
              "        [-13.7217, -12.3673, -15.8846,  ..., -19.0205, -18.6040, -13.4203]]), tensor([[-12.2772, -12.5668, -21.4743,  ..., -18.6896, -20.5301, -13.5424],\n",
              "        [-13.9401, -11.3949, -16.4059,  ..., -18.3805, -18.4310, -14.6433]]), tensor([[ -9.8580, -10.6353, -18.2077,  ..., -17.4738, -20.0558, -12.1486],\n",
              "        [-11.1597, -11.3314, -16.3664,  ..., -13.9170, -18.8905, -12.2782]]), tensor([[-11.3555, -10.5920, -15.9548,  ..., -14.9918, -17.6054, -11.2431],\n",
              "        [-11.8260, -10.9212, -16.2588,  ..., -14.0634, -18.8514, -11.2834]]), tensor([[-13.9413, -12.2929, -18.7696,  ..., -20.0654, -18.3165, -14.5566],\n",
              "        [-15.3641, -14.5727, -19.9497,  ..., -23.7027, -26.6946, -17.2803]]), tensor([[-13.1968, -11.9317, -18.5935,  ..., -16.1482, -19.0593, -13.0274],\n",
              "        [-12.0780, -12.7452, -20.0811,  ..., -20.8150, -23.3242, -15.7612]]), tensor([[-13.0265, -12.0616, -17.2557,  ..., -16.4247, -21.0164, -13.0647],\n",
              "        [ -7.1855,  -9.4349, -17.8712,  ..., -18.8740, -20.4259, -11.3578]]), tensor([[ -6.3949,  -8.8811, -14.9252,  ..., -20.2058, -21.2618,  -9.9299],\n",
              "        [ -6.6490,  -9.4966, -16.1087,  ..., -17.8912, -18.7007, -10.7657]]), tensor([[-13.2713, -13.5630, -13.8380,  ..., -25.6843, -28.6595,  -4.9778],\n",
              "        [-14.1281, -14.4748, -17.2882,  ..., -17.0913, -19.7101, -10.9737]]), tensor([[-16.1344, -13.1463, -18.5364,  ..., -39.1279, -46.5473, -12.5506],\n",
              "        [-13.8787, -13.4971, -17.8518,  ..., -22.7536, -15.0902, -14.9280]])), beam_indices=tensor([[ 0,  1,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0, -1, -1, -1, -1,\n",
              "         -1, -1],\n",
              "        [ 0,  1,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  0,  1, -1, -1, -1, -1,\n",
              "         -1, -1]]), attentions=None, hidden_states=None)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "input_ids = tokenizer(\"This is a scary story.\", return_tensors=\"pt\").input_ids\n",
        "# Adjust beams.\n",
        "outputs = model.generate(input_ids, num_beams=2, num_return_sequences=2, output_scores=True, length_penalty=0)\n",
        "outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['This',\n",
              "  'Ġis',\n",
              "  'Ġa',\n",
              "  'Ġscary',\n",
              "  'Ġstory',\n",
              "  '.',\n",
              "  'Ċ',\n",
              "  'Ċ',\n",
              "  'I',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġmake',\n",
              "  'Ġof',\n",
              "  'Ġit',\n",
              "  '.',\n",
              "  'Ċ',\n",
              "  'Ċ'],\n",
              " ['This',\n",
              "  'Ġis',\n",
              "  'Ġa',\n",
              "  'Ġscary',\n",
              "  'Ġstory',\n",
              "  '.',\n",
              "  'Ċ',\n",
              "  'Ċ',\n",
              "  'I',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġmake',\n",
              "  'Ġof',\n",
              "  'Ġit',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\"],\n",
              " 20)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(outputs[\"sequences\"][0]), tokenizer.convert_ids_to_tokens(outputs[\"sequences\"][1]), len(outputs[\"sequences\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((tensor([[-12.6087, -11.0740, -12.2497,  ..., -23.2802, -27.1528,  -5.2005],\n",
              "          [-12.6087, -11.0740, -12.2497,  ..., -23.2802, -27.1529,  -5.2005]]),\n",
              "  tensor([[-13.2075, -12.2395, -17.4515,  ..., -21.1790, -14.0837, -14.2907],\n",
              "          [-15.6107,  -8.7284, -13.3961,  ..., -32.1316, -39.7618, -10.3586]]),\n",
              "  tensor([[-12.7155,  -2.6471,  -6.8343,  ..., -26.4551, -30.0658, -13.9922],\n",
              "          [-14.3286, -12.3563, -17.0929,  ..., -19.5023, -18.9123, -14.7603]]),\n",
              "  tensor([[-13.0612,  -8.4863, -10.2133,  ..., -22.5126, -26.4204,  -9.6500],\n",
              "          [-12.1657, -11.5912, -15.1831,  ..., -20.0144, -13.1187, -12.7388]]),\n",
              "  tensor([[-13.2222, -10.9737, -17.2112,  ..., -20.3124, -13.6227, -14.1369],\n",
              "          [-13.7217, -12.3673, -15.8846,  ..., -19.0205, -18.6040, -13.4203]]),\n",
              "  tensor([[-12.2772, -12.5668, -21.4743,  ..., -18.6896, -20.5301, -13.5424],\n",
              "          [-13.9401, -11.3949, -16.4059,  ..., -18.3805, -18.4310, -14.6433]]),\n",
              "  tensor([[ -9.8580, -10.6353, -18.2077,  ..., -17.4738, -20.0558, -12.1486],\n",
              "          [-11.1597, -11.3314, -16.3664,  ..., -13.9170, -18.8905, -12.2782]]),\n",
              "  tensor([[-11.3555, -10.5920, -15.9548,  ..., -14.9918, -17.6054, -11.2431],\n",
              "          [-11.8260, -10.9212, -16.2588,  ..., -14.0634, -18.8514, -11.2834]]),\n",
              "  tensor([[-13.9413, -12.2929, -18.7696,  ..., -20.0654, -18.3165, -14.5566],\n",
              "          [-15.3641, -14.5727, -19.9497,  ..., -23.7027, -26.6946, -17.2803]]),\n",
              "  tensor([[-13.1968, -11.9317, -18.5935,  ..., -16.1482, -19.0593, -13.0274],\n",
              "          [-12.0780, -12.7452, -20.0811,  ..., -20.8150, -23.3242, -15.7612]]),\n",
              "  tensor([[-13.0265, -12.0616, -17.2557,  ..., -16.4247, -21.0164, -13.0647],\n",
              "          [ -7.1855,  -9.4349, -17.8712,  ..., -18.8740, -20.4259, -11.3578]]),\n",
              "  tensor([[ -6.3949,  -8.8811, -14.9252,  ..., -20.2058, -21.2618,  -9.9299],\n",
              "          [ -6.6490,  -9.4966, -16.1087,  ..., -17.8912, -18.7007, -10.7657]]),\n",
              "  tensor([[-13.2713, -13.5630, -13.8380,  ..., -25.6843, -28.6595,  -4.9778],\n",
              "          [-14.1281, -14.4748, -17.2882,  ..., -17.0913, -19.7101, -10.9737]]),\n",
              "  tensor([[-16.1344, -13.1463, -18.5364,  ..., -39.1279, -46.5473, -12.5506],\n",
              "          [-13.8787, -13.4971, -17.8518,  ..., -22.7536, -15.0902, -14.9280]])),\n",
              " 14,\n",
              " torch.Size([2, 50257]),\n",
              " tensor(314),\n",
              " ['ĠI'],\n",
              " [\"'m\"],\n",
              " [['ĠI'], [\"'m\"], ['\"'], ['I'], [\"'m\"]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[\"scores\"], len(outputs[\"scores\"]), outputs[\"scores\"][0].shape, outputs[\"scores\"][0][0].argmax(), tokenizer.convert_ids_to_tokens([outputs[\"scores\"][0][0].argmax().item()]), tokenizer.convert_ids_to_tokens([outputs[\"scores\"][1][0].argmax().item()]), [tokenizer.convert_ids_to_tokens([outputs[\"scores\"][i][0].argmax().item()]) for i in range(5)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
