{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GXs_RippsGL",
        "outputId": "f4e1eb25-be59-4067-fd19-9aadb741711b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
              " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
              " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
              " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "text = \"Replace me by any text you'd like.\"\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model(**encoded_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "211972"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import datasets\n",
        "train_10 = datasets.load_dataset(\"roneneldan/TinyStories\", split=\"train[:10%]\")\n",
        "len(train_10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00d7f7f5cf0c4d2ea525222ac965d770",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=8):   0%|          | 0/211972 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenization(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        max_length=model.config.max_position_embeddings,\n",
        "        truncation=True,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "\n",
        "# Batched tokenization. Can feed into model for outputs/logits to be softmaxed.\n",
        "ds = train_10.map(\n",
        "    tokenization,\n",
        "    batched=True,\n",
        "    num_proc=8,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids'],\n",
              "    num_rows: 211972\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
              " [3198,\n",
              "  1110,\n",
              "  11,\n",
              "  257,\n",
              "  1310,\n",
              "  2576,\n",
              "  3706,\n",
              "  20037,\n",
              "  1043,\n",
              "  257,\n",
              "  17598,\n",
              "  287,\n",
              "  607,\n",
              "  2119,\n",
              "  13,\n",
              "  1375,\n",
              "  2993,\n",
              "  340,\n",
              "  373,\n",
              "  2408,\n",
              "  284,\n",
              "  711,\n",
              "  351,\n",
              "  340,\n",
              "  780,\n",
              "  340,\n",
              "  373,\n",
              "  7786,\n",
              "  13,\n",
              "  20037,\n",
              "  2227,\n",
              "  284,\n",
              "  2648,\n",
              "  262,\n",
              "  17598,\n",
              "  351,\n",
              "  607,\n",
              "  1995,\n",
              "  11,\n",
              "  523,\n",
              "  673,\n",
              "  714,\n",
              "  34249,\n",
              "  257,\n",
              "  4936,\n",
              "  319,\n",
              "  607,\n",
              "  10147,\n",
              "  13,\n",
              "  198,\n",
              "  198,\n",
              "  43,\n",
              "  813,\n",
              "  1816,\n",
              "  284,\n",
              "  607,\n",
              "  1995,\n",
              "  290,\n",
              "  531,\n",
              "  11,\n",
              "  366,\n",
              "  29252,\n",
              "  11,\n",
              "  314,\n",
              "  1043,\n",
              "  428,\n",
              "  17598,\n",
              "  13,\n",
              "  1680,\n",
              "  345,\n",
              "  2648,\n",
              "  340,\n",
              "  351,\n",
              "  502,\n",
              "  290,\n",
              "  34249,\n",
              "  616,\n",
              "  10147,\n",
              "  1701,\n",
              "  2332,\n",
              "  1995,\n",
              "  13541,\n",
              "  290,\n",
              "  531,\n",
              "  11,\n",
              "  366,\n",
              "  5297,\n",
              "  11,\n",
              "  20037,\n",
              "  11,\n",
              "  356,\n",
              "  460,\n",
              "  2648,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  4259,\n",
              "  534,\n",
              "  10147,\n",
              "  526,\n",
              "  198,\n",
              "  198,\n",
              "  41631,\n",
              "  11,\n",
              "  484,\n",
              "  4888,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  384,\n",
              "  19103,\n",
              "  262,\n",
              "  4936,\n",
              "  319,\n",
              "  20037,\n",
              "  338,\n",
              "  10147,\n",
              "  13,\n",
              "  632,\n",
              "  373,\n",
              "  407,\n",
              "  2408,\n",
              "  329,\n",
              "  606,\n",
              "  780,\n",
              "  484,\n",
              "  547,\n",
              "  7373,\n",
              "  290,\n",
              "  5742,\n",
              "  1123,\n",
              "  584,\n",
              "  13,\n",
              "  2293,\n",
              "  484,\n",
              "  5201,\n",
              "  11,\n",
              "  20037,\n",
              "  26280,\n",
              "  607,\n",
              "  1995,\n",
              "  329,\n",
              "  7373,\n",
              "  262,\n",
              "  17598,\n",
              "  290,\n",
              "  18682,\n",
              "  607,\n",
              "  10147,\n",
              "  13,\n",
              "  1119,\n",
              "  1111,\n",
              "  2936,\n",
              "  3772,\n",
              "  780,\n",
              "  484,\n",
              "  550,\n",
              "  4888,\n",
              "  290,\n",
              "  3111,\n",
              "  1978,\n",
              "  13])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds[0][\"text\"], ds[0][\"input_ids\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(162, 132)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(ds[0][\"input_ids\"]), len(ds[0][\"text\"].split(\" \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'input_ids': [3041, 5372], 'attention_mask': [1, 1]},\n",
              " {'input_ids': [1326], 'attention_mask': [1]},\n",
              " {'input_ids': [1525], 'attention_mask': [1]},\n",
              " {'input_ids': [3041, 5372, 502, 416], 'attention_mask': [1, 1, 1, 1]},\n",
              " GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
              " })"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Replace\"), tokenizer(\"me\"), tokenizer(\"by\"), tokenizer(\"Replace me by\"), tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Re', 'place', 'Ġme', 'Ġby', 'Ġany', 'Ġtext'],\n",
              " ['Re', 'place', 'Ġme', 'Ġby', 'Ġany', 'Ġtext', 'Ġyou', \"'d\", 'Ġlike', '.'])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens([3041, 5372, 502, 416, 597, 2420]), tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'One day, a little girl named Lily found a needle in her room. She jumped out, grabbed a few items, pulled up the cover, got'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. It was an old tube in her back, with her eyes closed, her'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. She immediately had a good dream — a very different one.\\n\\nHer'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room. She was taken to the hospital and treated with antibiotics.\\n\\nShe has'},\n",
              " {'generated_text': 'One day, a little girl named Lily found a needle in her room.\\n\\n\"What,\" she said, holding out her hand to stop the'}]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator(\"One day, a little girl named Lily found a needle in her room.\", max_length=30, num_return_sequences=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,\n",
              " torch.Size([1, 10, 768]),\n",
              " 46.90026,\n",
              " -12.414162,\n",
              " 755,\n",
              " 1.0,\n",
              " 496)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "tmp = output.last_hidden_state[0][0].detach().numpy()\n",
        "type(output), output.last_hidden_state.shape, tmp.max(), tmp.min(), ((tmp > -1) & (tmp < 1)).sum(), scipy.special.softmax(tmp).sum(), scipy.special.softmax(tmp).argmax()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "input_ids = tokenizer(\"This is a scary story.\", return_tensors=\"pt\").input_ids\n",
        "# Adjust beams.\n",
        "outputs = model.generate(input_ids, num_beams=1, num_return_sequences=1, output_scores=True, max_length=50)  # , length_penalty=0)\n",
        "# outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['This',\n",
              "  'Ġis',\n",
              "  'Ġa',\n",
              "  'Ġscary',\n",
              "  'Ġstory',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġmake',\n",
              "  'Ġof',\n",
              "  'Ġit',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġdo',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġdo',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġdo',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\",\n",
              "  'Ġnot',\n",
              "  'Ġsure',\n",
              "  'Ġwhat',\n",
              "  'Ġto',\n",
              "  'Ġdo',\n",
              "  '.',\n",
              "  'ĠI',\n",
              "  \"'m\"],\n",
              " 50)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(outputs[\"sequences\"][0]), len(outputs[\"sequences\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((tensor([[-145.6897, -144.1550, -145.3307,  ..., -156.3612, -160.2338,\n",
              "           -138.2815]]),\n",
              "  tensor([[-163.4249, -162.4569, -167.6689,  ..., -171.3964, -164.3011,\n",
              "           -164.5081]]),\n",
              "  tensor([[-144.9066, -142.9343, -147.6708,  ..., -150.0803, -149.4902,\n",
              "           -145.3383]]),\n",
              "  tensor([[-140.2967, -139.6254, -149.2209,  ..., -146.1835, -147.8075,\n",
              "           -141.7704]]),\n",
              "  tensor([[-132.5165, -132.1021, -141.0429,  ..., -139.8127, -142.6759,\n",
              "           -134.9256]]),\n",
              "  tensor([[-111.0757, -109.2763, -116.0065,  ..., -114.7578, -117.3087,\n",
              "           -111.3735]]),\n",
              "  tensor([[-140.9013, -139.3839, -145.6808,  ..., -148.9513, -152.1206,\n",
              "           -142.7282]]),\n",
              "  tensor([[  1.7674,   3.1039,  -6.8053,  ...,  -7.1692, -10.1013,  -2.4498]]),\n",
              "  tensor([[ -93.5340,  -91.8021,  -98.7108,  ...,  -97.7029, -102.4045,\n",
              "            -94.2247]]),\n",
              "  tensor([[-63.0810, -62.7109, -71.2882,  ..., -76.9252, -77.8038, -66.9183]]),\n",
              "  tensor([[-152.7949, -150.4533, -152.4851,  ..., -164.2371, -168.3864,\n",
              "           -144.8275]]),\n",
              "  tensor([[-173.3242, -172.3147, -177.6132,  ..., -181.6857, -174.8456,\n",
              "           -174.3477]]),\n",
              "  tensor([[-151.1007, -149.3201, -155.1766,  ..., -155.8182, -157.0286,\n",
              "           -152.0680]]),\n",
              "  tensor([[-139.1875, -138.7737, -148.0509,  ..., -145.5913, -146.8996,\n",
              "           -141.3044]]),\n",
              "  tensor([[-126.6978, -126.5496, -134.8430,  ..., -134.7812, -136.9523,\n",
              "           -129.5877]]),\n",
              "  tensor([[-67.5671, -65.8676, -71.6485,  ..., -71.0539, -74.4426, -68.2455]]),\n",
              "  tensor([[-149.9191, -149.4861, -155.5775,  ..., -157.9061, -160.6301,\n",
              "           -151.8880]]),\n",
              "  tensor([[-100.7741, -100.6824, -110.3223,  ..., -113.5901, -117.0676,\n",
              "           -104.9759]]),\n",
              "  tensor([[-150.4214, -148.3536, -150.5787,  ..., -161.7386, -166.2231,\n",
              "           -143.6721]]),\n",
              "  tensor([[-162.0848, -160.8109, -165.5469,  ..., -170.7029, -164.7784,\n",
              "           -162.9942]]),\n",
              "  tensor([[-133.6604, -131.9517, -136.5162,  ..., -138.6325, -140.6644,\n",
              "           -134.8500]]),\n",
              "  tensor([[-125.1633, -125.0190, -132.3609,  ..., -132.8974, -133.8995,\n",
              "           -127.8775]]),\n",
              "  tensor([[-122.2001, -122.5588, -130.7172,  ..., -132.4507, -133.8875,\n",
              "           -125.5232]]),\n",
              "  tensor([[-17.7145, -16.7371, -22.0669,  ..., -24.0764, -27.1485, -19.1902]]),\n",
              "  tensor([[-152.0861, -152.2981, -158.3733,  ..., -161.6651, -163.1457,\n",
              "           -154.1291]]),\n",
              "  tensor([[-60.7564, -61.6427, -68.5640,  ..., -74.2791, -77.7477, -65.1751]]),\n",
              "  tensor([[-143.8793, -142.0834, -144.8149,  ..., -154.0271, -157.8700,\n",
              "           -137.8714]]),\n",
              "  tensor([[-151.3757, -150.5862, -155.0403,  ..., -160.8928, -154.5392,\n",
              "           -152.6460]]),\n",
              "  tensor([[-107.6044, -106.4628, -109.4626,  ..., -113.2537, -115.0039,\n",
              "           -108.5022]]),\n",
              "  tensor([[-72.5744, -73.0296, -77.6911,  ..., -82.0528, -81.9051, -74.8462]]),\n",
              "  tensor([[-83.2959, -84.2416, -90.9007,  ..., -96.4406, -95.8850, -86.8502]]),\n",
              "  tensor([[-38.7936, -37.9434, -43.2022,  ..., -47.4396, -51.0045, -40.2598]]),\n",
              "  tensor([[-122.4336, -123.5941, -128.8488,  ..., -133.8701, -136.3197,\n",
              "           -125.5017]]),\n",
              "  tensor([[10.4396,  9.2513,  3.6560,  ..., -4.2293, -5.9143,  6.8527]]),\n",
              "  tensor([[-116.3547, -114.7193, -117.2773,  ..., -125.6690, -128.9038,\n",
              "           -110.4134]]),\n",
              "  tensor([[-128.0965, -127.8225, -131.3176,  ..., -138.3671, -131.7014,\n",
              "           -129.7214]]),\n",
              "  tensor([[-58.1755, -57.5146, -59.5379,  ..., -64.8197, -65.7498, -59.1783]]),\n",
              "  tensor([[-144.1336, -144.9773, -149.4505,  ..., -154.2078, -154.9559,\n",
              "           -146.4093]]),\n",
              "  tensor([[-49.2001, -50.5160, -56.1193,  ..., -63.6827, -61.8156, -52.6162]]),\n",
              "  tensor([[-245.1466, -243.5657, -250.3485,  ..., -255.8201, -262.7745,\n",
              "           -245.8952]]),\n",
              "  tensor([[-177.1230, -179.1454, -184.7126,  ..., -192.2432, -196.9123,\n",
              "           -180.7561]]),\n",
              "  tensor([[31.2988, 30.2279, 25.0571,  ..., 16.6702, 15.7409, 28.3491]]),\n",
              "  tensor([[-82.0057, -80.4922, -82.6392,  ..., -90.6535, -93.1667, -75.7368]]),\n",
              "  tensor([[-73.4650, -73.1986, -75.8260,  ..., -84.3786, -77.8508, -75.2773]])),\n",
              " 44,\n",
              " torch.Size([1, 50257]),\n",
              " tensor(314),\n",
              " ['ĠI'],\n",
              " [\"'m\"],\n",
              " [['ĠI'],\n",
              "  [\"'m\"],\n",
              "  ['Ġnot'],\n",
              "  ['Ġsure'],\n",
              "  ['Ġwhat'],\n",
              "  ['Ġto'],\n",
              "  ['Ġmake'],\n",
              "  ['Ġof'],\n",
              "  ['Ġit'],\n",
              "  ['.'],\n",
              "  ['ĠI'],\n",
              "  [\"'m\"],\n",
              "  ['Ġnot'],\n",
              "  ['Ġsure'],\n",
              "  ['Ġwhat'],\n",
              "  ['Ġto'],\n",
              "  ['Ġdo'],\n",
              "  ['.'],\n",
              "  ['ĠI'],\n",
              "  [\"'m\"],\n",
              "  ['Ġnot'],\n",
              "  ['Ġsure'],\n",
              "  ['Ġwhat'],\n",
              "  ['Ġto'],\n",
              "  ['Ġdo'],\n",
              "  ['.'],\n",
              "  ['ĠI'],\n",
              "  [\"'m\"],\n",
              "  ['Ġnot'],\n",
              "  ['Ġsure'],\n",
              "  ['Ġwhat'],\n",
              "  ['Ġto'],\n",
              "  ['Ġdo'],\n",
              "  ['.'],\n",
              "  ['ĠI'],\n",
              "  [\"'m\"],\n",
              "  ['Ġnot'],\n",
              "  ['Ġsure'],\n",
              "  ['Ġwhat'],\n",
              "  ['Ġto'],\n",
              "  ['Ġdo'],\n",
              "  ['.'],\n",
              "  ['ĠI'],\n",
              "  [\"'m\"]])"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[\"scores\"], len(outputs[\"scores\"]), outputs[\"scores\"][0].shape, outputs[\"scores\"][0][0].argmax(), tokenizer.convert_ids_to_tokens([outputs[\"scores\"][0][0].argmax().item()]), tokenizer.convert_ids_to_tokens([outputs[\"scores\"][1][0].argmax().item()]), [tokenizer.convert_ids_to_tokens([outputs[\"scores\"][i][0].argmax().item()]) for i in range(44)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GreedySearchDecoderOnlyOutput' object has no attribute 'last_hidden_state'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/gilgamesh-local/persona-convergence/experiments/gpt2-scratch.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bscout-lab/home/gilgamesh-local/persona-convergence/experiments/gpt2-scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m outputs\u001b[39m.\u001b[39;49mlast_hidden_state\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GreedySearchDecoderOnlyOutput' object has no attribute 'last_hidden_state'"
          ]
        }
      ],
      "source": [
        "outputs.last_hidden_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
